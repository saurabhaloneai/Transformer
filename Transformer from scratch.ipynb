{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install BPEmb\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from bpemb import BPEmb"
      ],
      "metadata": {
        "id": "vWyjB-YNwTG_",
        "outputId": "91487884-c53a-42bc-fbb4-0685dc8e9f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting BPEmb\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from BPEmb) (4.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BPEmb) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from BPEmb) (2.27.1)\n",
            "Collecting sentencepiece (from BPEmb)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from BPEmb) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->BPEmb) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->BPEmb) (6.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->BPEmb) (3.4)\n",
            "Installing collected packages: sentencepiece, BPEmb\n",
            "Successfully installed BPEmb-0.3.4 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers From Scratch"
      ],
      "metadata": {
        "id": "MenE2varZEXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores) \n",
        "  return tf.matmul(weights, value), weights"
      ],
      "metadata": {
        "id": "7hpO6cGEN7HK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 3\n",
        "embed_dim = 4\n",
        "\n",
        "queries = np.random.rand(seq_len, embed_dim)\n",
        "keys = np.random.rand(seq_len, embed_dim)\n",
        "values = np.random.rand(seq_len, embed_dim)\n",
        "\n",
        "print(\"Queries:\\n\", queries)"
      ],
      "metadata": {
        "id": "WB2cDybgX5LZ",
        "outputId": "18f7e133-106e-43a9-ac4c-64810da22695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries:\n",
            " [[0.75214069 0.00748362 0.47214515 0.7548674 ]\n",
            " [0.41820822 0.14339506 0.27825209 0.67074722]\n",
            " [0.71687512 0.92480808 0.45985028 0.07183883]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, attn_weights = scaled_dot_product_attention(queries, keys, values)\n",
        "\n",
        "print(\"Output\\n\", output, \"\\n\")\n",
        "print(\"Weights\\n\", attn_weights)"
      ],
      "metadata": {
        "id": "pxKj56hNX5UO",
        "outputId": "2bcd75e8-eddb-4cda-d543-2e2cc3c9a0cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output\n",
            " tf.Tensor(\n",
            "[[0.5559631  0.34568194 0.62707156 0.2565914 ]\n",
            " [0.5634332  0.35336107 0.6214385  0.25486088]\n",
            " [0.5232962  0.32520714 0.6340663  0.25825214]], shape=(3, 4), dtype=float32) \n",
            "\n",
            "Weights\n",
            " tf.Tensor(\n",
            "[[0.37073195 0.3104373  0.3188307 ]\n",
            " [0.3726771  0.31980565 0.30751723]\n",
            " [0.33857176 0.29277173 0.3686565 ]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "seq_len = 3\n",
        "embed_dim = 12\n",
        "num_heads = 3\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "print(f\"Dimension of each head: {head_dim}\")"
      ],
      "metadata": {
        "id": "rJLyGtqbX3uW",
        "outputId": "a17a4d4f-f8a0-4660-c10a-8c6f59659bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of each head: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
        "print(\"Input shape: \", x.shape, \"\\n\")\n",
        "print(\"Input:\\n\", x)"
      ],
      "metadata": {
        "id": "7NcX3KBrX3uW",
        "outputId": "6e8ccf9f-7f78-48ea-b37b-c2931d7f89ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  (1, 3, 12) \n",
            "\n",
            "Input:\n",
            " [[[0.3 0.8 0.5 0.8 0.3 0.2 0.1 0.9 0.7 0.6 0.2 0.1]\n",
            "  [0.5 0.9 0.6 0.  0.3 0.7 0.1 0.6 0.9 0.6 0.6 0.5]\n",
            "  [0.5 0.5 0.7 0.8 0.5 0.4 0.1 0.2 0.9 0.  0.7 0.2]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The query weights for each head.\n",
        "wq0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The key weights for each head. \n",
        "wk0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The value weights for each head.\n",
        "wv0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv2 = np.random.rand(embed_dim, head_dim).round(1)"
      ],
      "metadata": {
        "id": "8zdg7rqrX3uX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The three sets of query weights (one for each head):\")\n",
        "print(\"wq0:\\n\", wq0)\n",
        "print(\"wq1:\\n\", wq1)\n",
        "print(\"wq2:\\n\", wq1)"
      ],
      "metadata": {
        "id": "QzMRHZooX3uX",
        "outputId": "049d0e06-7acb-4386-f886-c33e5deef230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three sets of query weights (one for each head):\n",
            "wq0:\n",
            " [[0.1 0.4 0.5 0.1]\n",
            " [0.1 1.  0.7 0.4]\n",
            " [0.7 0.1 1.  0.5]\n",
            " [0.2 0.4 0.3 0.1]\n",
            " [0.8 0.3 0.7 0.2]\n",
            " [0.4 0.8 0.4 0.8]\n",
            " [0.3 0.1 0.3 0.6]\n",
            " [0.7 0.1 0.1 0.4]\n",
            " [0.8 0.7 0.  0.3]\n",
            " [0.9 0.3 0.2 0.2]\n",
            " [0.6 0.1 0.2 0.7]\n",
            " [0.2 0.5 1.  0.1]]\n",
            "wq1:\n",
            " [[0.3 0.  0.5 0.7]\n",
            " [0.3 0.7 0.4 0.7]\n",
            " [0.6 0.5 0.9 0.3]\n",
            " [1.  0.9 0.8 0.9]\n",
            " [0.4 0.5 0.2 0.6]\n",
            " [0.3 0.4 1.  0.2]\n",
            " [0.6 0.8 0.4 0. ]\n",
            " [0.4 0.2 0.3 0.6]\n",
            " [0.5 0.3 0.  0.4]\n",
            " [0.9 0.9 0.4 0.2]\n",
            " [1.  0.1 0.5 0.8]\n",
            " [0.5 0.1 0.7 0.7]]\n",
            "wq2:\n",
            " [[0.3 0.  0.5 0.7]\n",
            " [0.3 0.7 0.4 0.7]\n",
            " [0.6 0.5 0.9 0.3]\n",
            " [1.  0.9 0.8 0.9]\n",
            " [0.4 0.5 0.2 0.6]\n",
            " [0.3 0.4 1.  0.2]\n",
            " [0.6 0.8 0.4 0. ]\n",
            " [0.4 0.2 0.3 0.6]\n",
            " [0.5 0.3 0.  0.4]\n",
            " [0.9 0.9 0.4 0.2]\n",
            " [1.  0.1 0.5 0.8]\n",
            " [0.5 0.1 0.7 0.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Geneated queries, keys, and values for the first head.\n",
        "q0 = np.dot(x, wq0)\n",
        "k0 = np.dot(x, wk0)\n",
        "v0 = np.dot(x, wv0)\n",
        "\n",
        "# Geneated queries, keys, and values for the second head.\n",
        "q1 = np.dot(x, wq1)\n",
        "k1 = np.dot(x, wk1)\n",
        "v1 = np.dot(x, wv1)\n",
        "\n",
        "# Geneated queries, keys, and values for the third head.\n",
        "q2 = np.dot(x, wq2)\n",
        "k2 = np.dot(x, wk2)\n",
        "v2 = np.dot(x, wv2)"
      ],
      "metadata": {
        "id": "NucbYNNSX3uX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q, K, and V for first head:\\n\")\n",
        "\n",
        "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
        "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
        "print(f\"v0 {v0.shape}:\\n\", v0)"
      ],
      "metadata": {
        "id": "NMcMmbkqX3uX",
        "outputId": "6f9ec6b7-19fb-4ce0-b16d-4a5686bb96f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q, K, and V for first head:\n",
            "\n",
            "q0 (1, 3, 4):\n",
            " [[[2.84 2.38 2.12 1.8 ]\n",
            "  [3.25 3.   2.8  2.49]\n",
            "  [2.66 2.39 2.44 2.02]]] \n",
            "\n",
            "k0 (1, 3, 4):\n",
            " [[[2.58 2.19 2.8  2.32]\n",
            "  [3.16 2.92 3.19 3.25]\n",
            "  [2.7  2.66 2.89 2.7 ]]] \n",
            "\n",
            "v0 (1, 3, 4):\n",
            " [[[2.56 2.92 2.18 2.84]\n",
            "  [3.04 3.99 2.08 3.  ]\n",
            "  [2.94 3.13 2.25 3.26]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_dot_product_attention(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "id": "i7tHIvXKX3uX",
        "outputId": "440beb6d-b519-4cf8-f522-b12e61c0af67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first attention head:  tf.Tensor(\n",
            "[[[3.0054321 3.8246443 2.1078758 3.0291328]\n",
            "  [3.0205224 3.8866928 2.0980802 3.0210824]\n",
            "  [3.0081773 3.835439  2.1062093 3.027895 ]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Attention weights from first head:  tf.Tensor(\n",
            "[[[0.04314128 0.8182604  0.13859834]\n",
            "  [0.02099406 0.8850018  0.09400418]\n",
            "  [0.03895167 0.8297894  0.13125895]]], shape=(1, 3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1, _ = scaled_dot_product_attention(q1, k1, v1)\n",
        "out2, _ = scaled_dot_product_attention(q2, k2, v2)\n",
        "\n",
        "print(\"Output from second attention head: \", out1, \"\\n\")\n",
        "print(\"Output from third attention head: \", out2,)"
      ],
      "metadata": {
        "id": "otnqbaDSqpJ7",
        "outputId": "b032acf6-f3a7-4244-b40a-ea026411ce94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from second attention head:  tf.Tensor(\n",
            "[[[1.8091758 2.7371604 3.9851067 2.9945998]\n",
            "  [1.80683   2.7380533 3.9870818 2.9951804]\n",
            "  [1.8097466 2.7364388 3.9845045 2.9953678]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Output from third attention head:  tf.Tensor(\n",
            "[[[3.7462146 3.5141351 3.336993  3.6518066]\n",
            "  [3.7518353 3.5321844 3.3548553 3.6597142]\n",
            "  [3.7440763 3.5070477 3.329955  3.6485972]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_a = np.concatenate((out0, out1, out2), axis=-1)\n",
        "print(f\"Combined output from all heads {combined_out_a.shape}:\")\n",
        "print(combined_out_a)\n",
        "\n",
        "# The final step would be to run combined_out_a through a linear/dense layer \n",
        "# for further processing."
      ],
      "metadata": {
        "id": "gmSv5trtt2v9",
        "outputId": "98682a06-55e9-4e0b-f53e-e1a52e323d23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined output from all heads (1, 3, 12):\n",
            "[[[3.0054321 3.8246443 2.1078758 3.0291328 1.8091758 2.7371604 3.9851067\n",
            "   2.9945998 3.7462146 3.5141351 3.336993  3.6518066]\n",
            "  [3.0205224 3.8866928 2.0980802 3.0210824 1.80683   2.7380533 3.9870818\n",
            "   2.9951804 3.7518353 3.5321844 3.3548553 3.6597142]\n",
            "  [3.0081773 3.835439  2.1062093 3.027895  1.8097466 2.7364388 3.9845045\n",
            "   2.9953678 3.7440763 3.5070477 3.329955  3.6485972]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query weights for first head: \\n\", wq0, \"\\n\")\n",
        "print(\"Query weights for second head: \\n\", wq1, \"\\n\")\n",
        "print(\"Query weights for third head: \\n\", wq2)"
      ],
      "metadata": {
        "id": "XoJmLAsUX3uX",
        "outputId": "90622ebd-ab32-4293-fab0-bc80c899f501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query weights for first head: \n",
            " [[0.1 0.4 0.5 0.1]\n",
            " [0.1 1.  0.7 0.4]\n",
            " [0.7 0.1 1.  0.5]\n",
            " [0.2 0.4 0.3 0.1]\n",
            " [0.8 0.3 0.7 0.2]\n",
            " [0.4 0.8 0.4 0.8]\n",
            " [0.3 0.1 0.3 0.6]\n",
            " [0.7 0.1 0.1 0.4]\n",
            " [0.8 0.7 0.  0.3]\n",
            " [0.9 0.3 0.2 0.2]\n",
            " [0.6 0.1 0.2 0.7]\n",
            " [0.2 0.5 1.  0.1]] \n",
            "\n",
            "Query weights for second head: \n",
            " [[0.3 0.  0.5 0.7]\n",
            " [0.3 0.7 0.4 0.7]\n",
            " [0.6 0.5 0.9 0.3]\n",
            " [1.  0.9 0.8 0.9]\n",
            " [0.4 0.5 0.2 0.6]\n",
            " [0.3 0.4 1.  0.2]\n",
            " [0.6 0.8 0.4 0. ]\n",
            " [0.4 0.2 0.3 0.6]\n",
            " [0.5 0.3 0.  0.4]\n",
            " [0.9 0.9 0.4 0.2]\n",
            " [1.  0.1 0.5 0.8]\n",
            " [0.5 0.1 0.7 0.7]] \n",
            "\n",
            "Query weights for third head: \n",
            " [[0.2 0.1 0.9 0.2]\n",
            " [0.  0.1 0.9 0.8]\n",
            " [0.7 0.5 0.8 1. ]\n",
            " [0.7 0.  0.9 0.4]\n",
            " [0.1 0.7 1.  0.2]\n",
            " [1.  0.3 0.2 0.5]\n",
            " [0.1 0.5 0.6 0.3]\n",
            " [0.5 0.9 0.7 0.4]\n",
            " [0.4 0.8 0.  0.7]\n",
            " [0.6 0.2 0.8 0.6]\n",
            " [0.6 0.3 0.6 0.4]\n",
            " [0.6 0.1 1.  0.3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wq = np.concatenate((wq0, wq1, wq2), axis=1)\n",
        "print(f\"Single query weight matrix {wq.shape}: \\n\", wq)"
      ],
      "metadata": {
        "id": "7jh6zeg1X3uX",
        "outputId": "e3982b58-c325-4562-dbf5-1f1c9f7c827b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single query weight matrix (12, 12): \n",
            " [[0.1 0.4 0.5 0.1 0.3 0.  0.5 0.7 0.2 0.1 0.9 0.2]\n",
            " [0.1 1.  0.7 0.4 0.3 0.7 0.4 0.7 0.  0.1 0.9 0.8]\n",
            " [0.7 0.1 1.  0.5 0.6 0.5 0.9 0.3 0.7 0.5 0.8 1. ]\n",
            " [0.2 0.4 0.3 0.1 1.  0.9 0.8 0.9 0.7 0.  0.9 0.4]\n",
            " [0.8 0.3 0.7 0.2 0.4 0.5 0.2 0.6 0.1 0.7 1.  0.2]\n",
            " [0.4 0.8 0.4 0.8 0.3 0.4 1.  0.2 1.  0.3 0.2 0.5]\n",
            " [0.3 0.1 0.3 0.6 0.6 0.8 0.4 0.  0.1 0.5 0.6 0.3]\n",
            " [0.7 0.1 0.1 0.4 0.4 0.2 0.3 0.6 0.5 0.9 0.7 0.4]\n",
            " [0.8 0.7 0.  0.3 0.5 0.3 0.  0.4 0.4 0.8 0.  0.7]\n",
            " [0.9 0.3 0.2 0.2 0.9 0.9 0.4 0.2 0.6 0.2 0.8 0.6]\n",
            " [0.6 0.1 0.2 0.7 1.  0.1 0.5 0.8 0.6 0.3 0.6 0.4]\n",
            " [0.2 0.5 1.  0.1 0.5 0.1 0.7 0.7 0.6 0.1 1.  0.3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wk = np.concatenate((wk0, wk1, wk2), axis=1)\n",
        "wv = np.concatenate((wv0, wv1, wv2), axis=1)\n",
        "\n",
        "print(f\"Single key weight matrix {wk.shape}:\\n\", wk, \"\\n\")\n",
        "print(f\"Single value weight matrix {wv.shape}:\\n\", wv)"
      ],
      "metadata": {
        "id": "xq2guuobX3uX",
        "outputId": "d6c08041-8729-4982-8ca7-0f569dd77c37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single key weight matrix (12, 12):\n",
            " [[0.2 0.6 0.2 0.4 0.4 0.1 0.6 0.6 0.2 0.5 0.1 0.5]\n",
            " [0.4 0.1 0.2 0.8 0.5 0.5 0.9 0.  0.4 0.9 0.3 0. ]\n",
            " [0.8 0.4 0.3 0.6 0.6 0.3 0.2 0.2 0.8 0.2 0.  0.1]\n",
            " [0.2 0.2 0.6 0.1 0.4 0.3 0.5 0.7 0.2 0.7 0.1 0. ]\n",
            " [0.7 1.  0.8 0.2 0.9 0.7 0.8 0.9 0.5 0.2 0.6 0.3]\n",
            " [0.7 0.5 0.3 0.6 0.7 0.9 0.7 0.4 0.1 0.  0.3 0.8]\n",
            " [0.6 0.9 0.4 0.9 0.5 0.5 0.4 0.7 0.  0.8 0.5 0.1]\n",
            " [0.5 0.1 0.7 0.1 0.8 0.2 0.2 0.8 0.9 0.8 0.9 0.3]\n",
            " [0.5 0.7 0.6 0.8 0.6 0.  0.6 0.4 0.4 0.8 0.6 0.4]\n",
            " [0.5 0.6 0.5 0.2 0.1 0.4 0.6 0.9 0.9 0.9 0.8 0.8]\n",
            " [0.6 0.5 0.8 0.5 0.6 0.  0.6 0.8 0.3 0.4 0.1 0.1]\n",
            " [0.1 0.4 1.  0.4 0.6 0.5 0.1 0.8 0.3 0.6 0.6 0.2]] \n",
            "\n",
            "Single value weight matrix (12, 12):\n",
            " [[0.3 0.4 0.4 0.  0.6 0.  0.1 0.5 0.9 0.5 0.4 0.1]\n",
            " [0.2 0.7 0.2 0.4 0.3 0.2 0.5 0.7 0.8 1.  0.6 0.8]\n",
            " [0.5 0.6 0.6 1.  0.2 0.4 0.5 0.5 0.7 0.3 0.6 1. ]\n",
            " [0.3 0.  0.6 0.8 0.8 0.7 1.  0.8 1.  0.3 0.5 0.5]\n",
            " [0.6 0.6 0.1 0.4 0.4 0.6 0.4 0.2 0.9 0.4 0.  0.7]\n",
            " [0.2 0.8 0.1 0.3 0.  0.7 0.7 0.2 0.1 0.5 1.  0.5]\n",
            " [0.6 0.6 0.1 0.9 0.1 0.8 0.6 0.8 0.5 0.5 0.7 0.2]\n",
            " [0.7 0.6 0.5 0.1 0.2 0.6 0.8 0.1 0.1 0.4 0.4 0.9]\n",
            " [0.9 0.8 0.8 0.9 0.4 0.3 0.9 0.8 0.9 0.6 0.2 0.1]\n",
            " [0.1 0.4 0.  0.4 0.5 0.9 0.6 0.1 0.4 0.6 0.9 0.9]\n",
            " [0.9 0.8 0.1 0.6 0.2 0.7 1.  0.9 0.5 1.  0.8 0.3]\n",
            " [0.4 0.4 0.3 0.3 0.1 0.  0.6 0.3 0.8 0.1 0.3 0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_s = np.dot(x, wq)\n",
        "k_s = np.dot(x, wk)\n",
        "v_s = np.dot(x, wv)"
      ],
      "metadata": {
        "id": "UQ5i98bLX3uX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query vectors using a single weight matrix {q_s.shape}:\\n\", q_s)"
      ],
      "metadata": {
        "id": "H-qKM3jZr242",
        "outputId": "d48f269b-85c6-4bc3-b8f3-53ade4a5959d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vectors using a single weight matrix (1, 3, 12):\n",
            " [[[2.84 2.38 2.12 1.8  3.17 2.8  2.54 3.03 2.48 2.24 3.84 3.03]\n",
            "  [3.25 3.   2.8  2.49 3.25 2.48 3.02 3.15 2.94 2.52 4.   3.48]\n",
            "  [2.66 2.39 2.44 2.02 3.23 2.31 2.81 3.19 2.61 2.1  3.58 2.9 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "id": "FKXYVHbJvnGp",
        "outputId": "b5c380fc-6073-4855-f715-ea7a4dcc0f07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[2.84 2.38 2.12 1.8 ]\n",
            "  [3.25 3.   2.8  2.49]\n",
            "  [2.66 2.39 2.44 2.02]]] \n",
            "\n",
            "[[[3.17 2.8  2.54 3.03]\n",
            "  [3.25 2.48 3.02 3.15]\n",
            "  [3.23 2.31 2.81 3.19]]] \n",
            "\n",
            "[[[2.48 2.24 3.84 3.03]\n",
            "  [2.94 2.52 4.   3.48]\n",
            "  [2.61 2.1  3.58 2.9 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we can achieve the same thing by passing -1 instead of seq_len.\n",
        "q_s_reshaped = tf.reshape(q_s, (batch_size, seq_len, num_heads, head_dim))\n",
        "print(f\"Combined queries: {q_s.shape}\\n\", q_s, \"\\n\")\n",
        "print(f\"Reshaped into separate heads: {q_s_reshaped.shape}\\n\", q_s_reshaped)"
      ],
      "metadata": {
        "id": "d3iHh7XxX3uY",
        "outputId": "08050da8-a2c1-4798-92d8-3ae49e767f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined queries: (1, 3, 12)\n",
            " [[[2.84 2.38 2.12 1.8  3.17 2.8  2.54 3.03 2.48 2.24 3.84 3.03]\n",
            "  [3.25 3.   2.8  2.49 3.25 2.48 3.02 3.15 2.94 2.52 4.   3.48]\n",
            "  [2.66 2.39 2.44 2.02 3.23 2.31 2.81 3.19 2.61 2.1  3.58 2.9 ]]] \n",
            "\n",
            "Reshaped into separate heads: (1, 3, 3, 4)\n",
            " tf.Tensor(\n",
            "[[[[2.84 2.38 2.12 1.8 ]\n",
            "   [3.17 2.8  2.54 3.03]\n",
            "   [2.48 2.24 3.84 3.03]]\n",
            "\n",
            "  [[3.25 3.   2.8  2.49]\n",
            "   [3.25 2.48 3.02 3.15]\n",
            "   [2.94 2.52 4.   3.48]]\n",
            "\n",
            "  [[2.66 2.39 2.44 2.02]\n",
            "   [3.23 2.31 2.81 3.19]\n",
            "   [2.61 2.1  3.58 2.9 ]]]], shape=(1, 3, 3, 4), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_s_transposed = tf.transpose(q_s_reshaped, perm=[0, 2, 1, 3]).numpy()\n",
        "print(f\"Queries transposed into \\\"separate\\\" heads {q_s_transposed.shape}:\\n\", \n",
        "      q_s_transposed)"
      ],
      "metadata": {
        "id": "6Vv3kV3jX3uY",
        "outputId": "434e0253-628d-49cc-e2ef-13d8d768714e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries transposed into \"separate\" heads (1, 3, 3, 4):\n",
            " [[[[2.84 2.38 2.12 1.8 ]\n",
            "   [3.25 3.   2.8  2.49]\n",
            "   [2.66 2.39 2.44 2.02]]\n",
            "\n",
            "  [[3.17 2.8  2.54 3.03]\n",
            "   [3.25 2.48 3.02 3.15]\n",
            "   [3.23 2.31 2.81 3.19]]\n",
            "\n",
            "  [[2.48 2.24 3.84 3.03]\n",
            "   [2.94 2.52 4.   3.48]\n",
            "   [2.61 2.1  3.58 2.9 ]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The separate per-head query matrices from before: \")\n",
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "id": "ZMLEBmtowQ02",
        "outputId": "39f7ba24-e02d-4bbc-876f-3a5999927487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The separate per-head query matrices from before: \n",
            "[[[2.84 2.38 2.12 1.8 ]\n",
            "  [3.25 3.   2.8  2.49]\n",
            "  [2.66 2.39 2.44 2.02]]] \n",
            "\n",
            "[[[3.17 2.8  2.54 3.03]\n",
            "  [3.25 2.48 3.02 3.15]\n",
            "  [3.23 2.31 2.81 3.19]]] \n",
            "\n",
            "[[[2.48 2.24 3.84 3.03]\n",
            "  [2.94 2.52 4.   3.48]\n",
            "  [2.61 2.1  3.58 2.9 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_s_transposed = tf.transpose(tf.reshape(k_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()\n",
        "v_s_transposed = tf.transpose(tf.reshape(v_s, (batch_size, -1, num_heads, head_dim)), perm=[0, 2, 1, 3]).numpy()\n",
        "\n",
        "print(f\"Keys for all heads in a single matrix {k_s.shape}: \\n\", k_s_transposed, \"\\n\")\n",
        "print(f\"Values for all heads in a single matrix {v_s.shape}: \\n\", v_s_transposed)"
      ],
      "metadata": {
        "id": "vauGkBv3X3uY",
        "outputId": "a56e55b6-6309-43de-dffb-c35cec5d9837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys for all heads in a single matrix (1, 3, 12): \n",
            " [[[[2.58 2.19 2.8  2.32]\n",
            "   [3.16 2.92 3.19 3.25]\n",
            "   [2.7  2.66 2.89 2.7 ]]\n",
            "\n",
            "  [[2.98 1.73 2.91 3.04]\n",
            "   [3.56 2.18 3.43 3.3 ]\n",
            "   [3.21 1.65 3.03 2.92]]\n",
            "\n",
            "  [[2.83 3.63 2.43 1.53]\n",
            "   [2.93 3.6  2.68 2.15]\n",
            "   [2.12 2.86 1.66 1.33]]]] \n",
            "\n",
            "Values for all heads in a single matrix (1, 3, 12): \n",
            " [[[[2.56 2.92 2.18 2.84]\n",
            "   [3.04 3.99 2.08 3.  ]\n",
            "   [2.94 3.13 2.25 3.26]]\n",
            "\n",
            "  [[2.1  2.75 3.77 2.7 ]\n",
            "   [1.77 2.76 4.02 2.99]\n",
            "   [2.   2.48 3.78 3.28]]\n",
            "\n",
            "  [[3.54 2.96 2.8  3.46]\n",
            "   [3.79 3.64 3.46 3.7 ]\n",
            "   [4.02 2.99 2.67 2.76]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_heads_output, all_attn_weights = scaled_dot_product_attention(q_s_transposed, \n",
        "                                                                  k_s_transposed, \n",
        "                                                                  v_s_transposed)\n",
        "print(\"Self attention output:\\n\", all_heads_output)"
      ],
      "metadata": {
        "id": "hIElo1ObX3uY",
        "outputId": "f02a9f36-b113-42b9-b975-90138823d88f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self attention output:\n",
            " tf.Tensor(\n",
            "[[[[3.0054321 3.8246443 2.1078758 3.0291328]\n",
            "   [3.0205224 3.8866928 2.0980802 3.0210824]\n",
            "   [3.0081773 3.835439  2.1062093 3.027895 ]]\n",
            "\n",
            "  [[1.8091758 2.7371604 3.9851067 2.9945998]\n",
            "   [1.80683   2.7380533 3.9870818 2.9951804]\n",
            "   [1.8097466 2.7364388 3.9845045 2.9953678]]\n",
            "\n",
            "  [[3.7462146 3.5141351 3.336993  3.6518066]\n",
            "   [3.7518353 3.5321844 3.3548553 3.6597142]\n",
            "   [3.7440763 3.5070477 3.329955  3.6485972]]]], shape=(1, 3, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Per head outputs from using separate sets of weights per head:\")\n",
        "print(out0, \"\\n\")\n",
        "print(out1, \"\\n\")\n",
        "print(out2)"
      ],
      "metadata": {
        "id": "bXIB_z11xsh7",
        "outputId": "4f56413f-295b-4686-f969-1198936f4c90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per head outputs from using separate sets of weights per head:\n",
            "tf.Tensor(\n",
            "[[[3.0054321 3.8246443 2.1078758 3.0291328]\n",
            "  [3.0205224 3.8866928 2.0980802 3.0210824]\n",
            "  [3.0081773 3.835439  2.1062093 3.027895 ]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[1.8091758 2.7371604 3.9851067 2.9945998]\n",
            "  [1.80683   2.7380533 3.9870818 2.9951804]\n",
            "  [1.8097466 2.7364388 3.9845045 2.9953678]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[3.7462146 3.5141351 3.336993  3.6518066]\n",
            "  [3.7518353 3.5321844 3.3548553 3.6597142]\n",
            "  [3.7440763 3.5070477 3.329955  3.6485972]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_b = tf.reshape(tf.transpose(all_heads_output, perm=[0, 2, 1, 3]), \n",
        "                            shape=(batch_size, seq_len, embed_dim))\n",
        "print(\"Final output from using single query, key, value matrices:\\n\", \n",
        "      combined_out_b, \"\\n\")\n",
        "print(\"Final output from using separate query, key, value matrices per head:\\n\", \n",
        "      combined_out_a)"
      ],
      "metadata": {
        "id": "9lWtCPk1wuod",
        "outputId": "5ad3c035-fde4-4a79-93e9-952e7881c881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output from using single query, key, value matrices:\n",
            " tf.Tensor(\n",
            "[[[3.0054321 3.8246443 2.1078758 3.0291328 1.8091758 2.7371604 3.9851067\n",
            "   2.9945998 3.7462146 3.5141351 3.336993  3.6518066]\n",
            "  [3.0205224 3.8866928 2.0980802 3.0210824 1.80683   2.7380533 3.9870818\n",
            "   2.9951804 3.7518353 3.5321844 3.3548553 3.6597142]\n",
            "  [3.0081773 3.835439  2.1062093 3.027895  1.8097466 2.7364388 3.9845045\n",
            "   2.9953678 3.7440763 3.5070477 3.329955  3.6485972]]], shape=(1, 3, 12), dtype=float32) \n",
            "\n",
            "Final output from using separate query, key, value matrices per head:\n",
            " [[[3.0054321 3.8246443 2.1078758 3.0291328 1.8091758 2.7371604 3.9851067\n",
            "   2.9945998 3.7462146 3.5141351 3.336993  3.6518066]\n",
            "  [3.0205224 3.8866928 2.0980802 3.0210824 1.80683   2.7380533 3.9870818\n",
            "   2.9951804 3.7518353 3.5321844 3.3548553 3.6597142]\n",
            "  [3.0081773 3.835439  2.1062093 3.027895  1.8097466 2.7364388 3.9845045\n",
            "   2.9953678 3.7440763 3.5070477 3.329955  3.6485972]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.d_head = self.d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    # Linear layer to generate the final output.\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "  \n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
        "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
        "  \n",
        "  def merge_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "    qs = self.wq(q)\n",
        "    ks = self.wk(k)\n",
        "    vs = self.wv(v)\n",
        "\n",
        "    qs = self.split_heads(qs)\n",
        "    ks = self.split_heads(ks)\n",
        "    vs = self.split_heads(vs)\n",
        "\n",
        "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
        "    output = self.merge_heads(output)\n",
        "\n",
        "    return self.dense(output), attn_weights\n"
      ],
      "metadata": {
        "id": "Sd_IgJI34vP4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mhsa = MultiHeadSelfAttention(12, 3)\n",
        "\n",
        "output, attn_weights = mhsa(x, x, x, None)\n",
        "print(f\"MHSA output{output.shape}:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "nuvv-8cg6owq",
        "outputId": "2cf5c285-e351-44ab-efff-fd75c2e46995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MHSA output(1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[ 0.2450927  -0.7318045  -0.07346617 -0.04063541  0.20350856\n",
            "    0.65724885 -0.1746743   0.02042755  0.6685578   0.08295882\n",
            "    0.8883524   0.01280902]\n",
            "  [ 0.2583107  -0.73946834 -0.07798081 -0.06133163  0.20616147\n",
            "    0.6644884  -0.1791845   0.02522293  0.6920126   0.09343398\n",
            "    0.921222   -0.0015862 ]\n",
            "  [ 0.24218246 -0.73264205 -0.07206094 -0.04281443  0.20945096\n",
            "    0.6538497  -0.17880386  0.01817051  0.6598376   0.08123702\n",
            "    0.89126253  0.01403455]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Block"
      ],
      "metadata": {
        "id": "uAk-GG2yMM59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(d_model, hidden_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "mN5B0vduMM9a"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our encoder block containing all the layers and steps from the preceding illustration (plus dropout)."
      ],
      "metadata": {
        "id": "4FrRAMJFDnVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x, training, mask):\n",
        "    mhsa_output, attn_weights = self.mhsa(x, x, x, mask)\n",
        "    mhsa_output = self.dropout1(mhsa_output, training=training)\n",
        "    mhsa_output = self.layernorm1(x + mhsa_output)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    output = self.layernorm2(mhsa_output + ffn_output)\n",
        "\n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "q8uu0mISAb0n"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_block = EncoderBlock(12, 3, 48)\n",
        "\n",
        "block_output,  _ = encoder_block(x, True, None)\n",
        "print(f\"Output from single encoder block {block_output.shape}:\")\n",
        "print(block_output)"
      ],
      "metadata": {
        "id": "vBnumPJ7C7Jj",
        "outputId": "39cd6da9-4173-4671-c688-988b9c46e0a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from single encoder block (1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[-1.2853216  -0.47298345  1.1163142   0.28988305 -0.15201803\n",
            "    0.714354   -0.44642696  2.18815    -0.98428893  0.35164022\n",
            "   -1.4659402   0.14663789]\n",
            "  [-1.4989055  -0.36181933  1.5024801  -0.05960907 -0.21404044\n",
            "    1.3806804  -0.81460243  1.3942987  -1.0500358   0.3370352\n",
            "   -1.14041     0.52492774]\n",
            "  [-1.2724154  -1.0890633   1.3531497   1.2134061   0.09007032\n",
            "    1.2355703  -0.6275463   1.3311381  -1.1320009   0.10560008\n",
            "   -0.88254625 -0.32536274]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word and Positional Embeddings"
      ],
      "metadata": {
        "id": "I5z32v2QKYdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English tokenizer.\n",
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "metadata": {
        "id": "nmMOHYDEKdvQ",
        "outputId": "8697cd0d-eeb9-487f-8a93-37ffa024eff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 493981.83B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3784656/3784656 [00:01<00:00, 2681792.28B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpemb_vocab_size, bpemb_embed_size = bpemb_en.vectors.shape\n",
        "print(\"Vocabulary size:\", bpemb_vocab_size)\n",
        "print(\"Embedding size:\", bpemb_embed_size)"
      ],
      "metadata": {
        "id": "FhtnbTmdH6jU",
        "outputId": "5e8e2139-c176-479a-ce9b-902a025d9074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Embedding size: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding for the word \"car\".\n",
        "bpemb_en.vectors[bpemb_en.words.index('car')]"
      ],
      "metadata": {
        "id": "vKvODSJDIdt0",
        "outputId": "0e20ae85-9219-40dc-f9c9-2abcd1a3c565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.305548, -0.325598, -0.134716, -0.078735, -0.660545,  0.076211,\n",
              "       -0.735487,  0.124533, -0.294402,  0.459688,  0.030137,  0.174041,\n",
              "       -0.224223,  0.486189, -0.504649, -0.459699,  0.315747,  0.477885,\n",
              "        0.091398,  0.427867,  0.016524, -0.076833, -0.899727,  0.493158,\n",
              "       -0.022309, -0.422785, -0.154148,  0.204981,  0.379834,  0.070588,\n",
              "        0.196073, -0.368222,  0.473406,  0.007409,  0.004303, -0.007823,\n",
              "       -0.19103 , -0.202509,  0.109878, -0.224521, -0.35741 , -0.611633,\n",
              "        0.329958, -0.212956, -0.497499, -0.393839, -0.130101, -0.216903,\n",
              "       -0.105595, -0.076007, -0.483942, -0.139704, -0.161647,  0.136985,\n",
              "        0.415363, -0.360143,  0.038601, -0.078804, -0.030421,  0.324129,\n",
              "        0.223378, -0.523636, -0.048317, -0.032248, -0.117367,  0.470519,\n",
              "        0.225816, -0.222065, -0.225007, -0.165904, -0.334389, -0.20157 ,\n",
              "        0.572352, -0.268794,  0.301929, -0.005563,  0.387491,  0.261031,\n",
              "       -0.11613 ,  0.074982, -0.008433,  0.259987, -0.099893, -0.268875,\n",
              "       -0.054047, -0.534776, -0.111101, -0.051742,  0.214114,  0.04293 ,\n",
              "        0.039873, -0.453112,  0.087382, -0.333201, -0.034079, -0.833045,\n",
              "        0.155232, -1.132393, -0.294766,  0.327572], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"Where can I find a pizzeria?\"\n",
        "tokens = bpemb_en.encode(sample_sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "AIgpfG3hKjbZ",
        "outputId": "8c60291f-5d82-4e0a-bb0b-6ed8b441eace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_seq = np.array(bpemb_en.encode_ids(\"Where can I find a pizzeria?\"))\n",
        "print(token_seq)"
      ],
      "metadata": {
        "id": "grMR-DHEKjWx",
        "outputId": "7e29cabe-e816-46a8-9ea0-c3cdf46c4de2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 571  280  386 1934    4   24  248 4339  177 9967]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embed = tf.keras.layers.Embedding(bpemb_vocab_size, embed_dim)\n",
        "token_embeddings = token_embed(token_seq)\n",
        "\n",
        "# The untrained embeddings for our sample sentence.\n",
        "print(\"Embeddings for: \", sample_sentence)\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "id": "UO7eOOrWKjSc",
        "outputId": "e4dff79d-0744-4afc-bdd3-7ddb6ebd26b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings for:  Where can I find a pizzeria?\n",
            "tf.Tensor(\n",
            "[[ 0.021247   -0.01948299 -0.04639702 -0.02132328  0.03410709 -0.00271456\n",
            "  -0.01825256  0.00657606 -0.03765022  0.02265502 -0.02121254 -0.0438798 ]\n",
            " [ 0.00857258 -0.01821322 -0.02282575 -0.0406186  -0.00255258 -0.04327289\n",
            "   0.04462032 -0.01243747  0.02584079  0.01054467  0.03417119 -0.02772656]\n",
            " [-0.04897821  0.03801889 -0.02050481  0.04144004 -0.03804501 -0.03834953\n",
            "  -0.03759488 -0.00296963 -0.03374429  0.01062299  0.01433483 -0.01363866]\n",
            " [ 0.00094473 -0.0388581   0.00746418 -0.04288788 -0.02629236 -0.02000649\n",
            "   0.01079553 -0.04675244  0.00462361 -0.04766339  0.02018405 -0.04500157]\n",
            " [-0.04792075  0.01269305 -0.04440744 -0.00462649 -0.03221427 -0.00901992\n",
            "   0.00595009  0.04646336 -0.01926882 -0.02931499 -0.02091855  0.02064309]\n",
            " [-0.0341584  -0.02598354  0.04016859 -0.014705   -0.01763175  0.03261962\n",
            "  -0.03887844 -0.00059711  0.00582775  0.01827694 -0.01941589 -0.0366724 ]\n",
            " [-0.0433309   0.02875911  0.02125175 -0.03237639 -0.02858753 -0.04741376\n",
            "   0.00621814 -0.00696132  0.04157242  0.01795231 -0.03376371  0.00933789]\n",
            " [-0.00371218 -0.03948849 -0.0489884   0.02823508 -0.00334175  0.00815926\n",
            "  -0.03409461 -0.03231185 -0.03814219 -0.01399152 -0.01644877  0.00604003]\n",
            " [ 0.00960413  0.00449723  0.02159694  0.00469196 -0.02308701 -0.03653352\n",
            "  -0.01631906 -0.03981483 -0.0389882   0.04540639 -0.04598868  0.0392744 ]\n",
            " [ 0.03991746  0.0460164  -0.04732884  0.00146548 -0.02880381  0.01882415\n",
            "  -0.04895698  0.02596853 -0.03038211  0.00798104 -0.03936472  0.0051906 ]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 256\n",
        "pos_embed = tf.keras.layers.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "# Generate ids for each position of the token sequence.\n",
        "pos_idx = tf.range(len(token_seq))\n",
        "print(pos_idx)"
      ],
      "metadata": {
        "id": "pcurqcv3KjNY",
        "outputId": "80da070a-5f25-4f7f-e761-a702723376b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These are our positon embeddings.\n",
        "position_embeddings = pos_embed(pos_idx)\n",
        "print(\"Position embeddings for the input sequence\\n\", position_embeddings)"
      ],
      "metadata": {
        "id": "6vIgau8YMTgi",
        "outputId": "f693af47-b022-4c86-8878-2355cdc15e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position embeddings for the input sequence\n",
            " tf.Tensor(\n",
            "[[-0.02627332  0.00494404 -0.00553044  0.00529874 -0.00445414 -0.03119564\n",
            "   0.04908427  0.00199108 -0.00782778 -0.00158896  0.03763149 -0.02136957]\n",
            " [-0.04744848 -0.00125855 -0.03048106  0.03316036 -0.00197734  0.00174314\n",
            "   0.0232848   0.02844452  0.04862762 -0.04100418  0.03514743 -0.0242133 ]\n",
            " [ 0.01328549 -0.00645056 -0.0093023   0.018146   -0.04506398 -0.03319913\n",
            "  -0.02150148 -0.02230905  0.00856518 -0.02947686  0.02523848 -0.03266766]\n",
            " [ 0.00449647 -0.02950015 -0.04376494  0.04214707 -0.03811678 -0.02868011\n",
            "  -0.03589477  0.03419762  0.04107979  0.02873271  0.01633034 -0.0194487 ]\n",
            " [ 0.02935529 -0.04177836 -0.04488919 -0.03773079 -0.01054424  0.04607414\n",
            "   0.03547266  0.00316938  0.04814469 -0.0112382  -0.0283497  -0.00060313]\n",
            " [-0.04935927  0.0287292  -0.03315626 -0.03452907  0.02696041 -0.04182808\n",
            "  -0.03011742  0.01208083  0.00889037  0.01043107 -0.03231344 -0.0325183 ]\n",
            " [-0.0472222  -0.04470063  0.01623067 -0.00702249 -0.01612936 -0.03904019\n",
            "  -0.01345099  0.02311042  0.01976276 -0.00274708  0.03572085 -0.04635547]\n",
            " [-0.02418422  0.0211895  -0.02034953 -0.02778579  0.00171686  0.04838279\n",
            "   0.04530198 -0.00882737  0.00322243  0.04546907  0.02781317 -0.0005365 ]\n",
            " [ 0.0048847   0.00657017 -0.03835946 -0.04080499 -0.03235554  0.02851528\n",
            "  -0.01078932  0.02434916  0.01370808 -0.01102678  0.01042068 -0.04462115]\n",
            " [-0.02526927  0.00705897 -0.01722537 -0.00663795  0.0240449   0.01195043\n",
            "   0.00634839 -0.01425755  0.01906288 -0.00811549 -0.04773686 -0.01406661]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = token_embeddings + position_embeddings\n",
        "print(\"Input to the initial encoder block:\\n\", input)"
      ],
      "metadata": {
        "id": "K6x9JVlTKjIi",
        "outputId": "bb4027e2-0bdf-4e0f-ed2a-e109f6333eaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the initial encoder block:\n",
            " tf.Tensor(\n",
            "[[-0.00502633 -0.01453896 -0.05192746 -0.01602453  0.02965296 -0.0339102\n",
            "   0.03083171  0.00856714 -0.045478    0.02106606  0.01641894 -0.06524936]\n",
            " [-0.0388759  -0.01947178 -0.05330682 -0.00745824 -0.00452992 -0.04152975\n",
            "   0.06790512  0.01600704  0.0744684  -0.03045951  0.06931862 -0.05193986]\n",
            " [-0.03569272  0.03156834 -0.0298071   0.05958605 -0.08310899 -0.07154866\n",
            "  -0.05909636 -0.02527869 -0.02517911 -0.01885387  0.03957331 -0.04630632]\n",
            " [ 0.0054412  -0.06835826 -0.03630076 -0.00074081 -0.06440914 -0.0486866\n",
            "  -0.02509924 -0.01255482  0.0457034  -0.01893068  0.03651439 -0.06445026]\n",
            " [-0.01856546 -0.02908531 -0.08929662 -0.04235728 -0.04275851  0.03705422\n",
            "   0.04142275  0.04963274  0.02887586 -0.04055319 -0.04926825  0.02003996]\n",
            " [-0.08351767  0.00274567  0.00701233 -0.04923407  0.00932866 -0.00920846\n",
            "  -0.06899586  0.01148372  0.01471812  0.02870801 -0.05172934 -0.06919071]\n",
            " [-0.09055309 -0.01594152  0.03748242 -0.03939888 -0.04471689 -0.08645394\n",
            "  -0.00723286  0.01614909  0.06133518  0.01520523  0.00195714 -0.03701758]\n",
            " [-0.02789639 -0.01829899 -0.06933793  0.00044929 -0.00162488  0.05654204\n",
            "   0.01120737 -0.04113921 -0.03491976  0.03147755  0.0113644   0.00550352]\n",
            " [ 0.01448883  0.0110674  -0.01676252 -0.03611303 -0.05544255 -0.00801824\n",
            "  -0.02710838 -0.01546567 -0.02528011  0.03437961 -0.035568   -0.00534675]\n",
            " [ 0.01464819  0.05307537 -0.06455421 -0.00517248 -0.00475892  0.03077458\n",
            "  -0.04260859  0.01171098 -0.01131923 -0.00013445 -0.08710158 -0.00887601]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "LDctrWODMNG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(src_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    # The original Attention Is All You Need paper applied dropout to the\n",
        "    # input before feeding it to the first encoder block.\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    # Create encoder blocks.\n",
        "    self.blocks = [EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) \n",
        "    for _ in range(num_blocks)]\n",
        "  \n",
        "  def call(self, input, training, mask):\n",
        "    token_embeds = self.token_embed(input)\n",
        "\n",
        "    # Generate position indices for a batch of input sequences.\n",
        "    num_pos = input.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, input.shape)\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    # Run input through successive encoder blocks.\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(x, training, mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "NinUihSpC6K-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch of 3 sequences, each of length 10 (10 is also the \n",
        "# maximum sequence length in this case).\n",
        "seqs = np.random.randint(0, 10000, size=(3, 10))\n",
        "print(seqs.shape)\n",
        "print(seqs)"
      ],
      "metadata": {
        "id": "Cllud1-mJhNi",
        "outputId": "c1f6da60-92f0-4b08-c659-6ebc5b94f9c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[4543 6043 1740 5076 3654 5748 6803 8059 2385  117]\n",
            " [3776 4692 2924 9663 6247 1636 3160 2651 3796 5222]\n",
            " [ 898 1187 1663 3801  600  853 4573 8389 2772 8480]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.resize(np.arange(seqs.shape[1]), seqs.shape[0] * seqs.shape[1])\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "id": "WgfMkY6fk4I4",
        "outputId": "db6dae1e-ead0-4e86-d409-89bfe2bd35e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ids = np.reshape(pos_ids, (3, 10))\n",
        "print(pos_ids.shape)\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "id": "ah0t-pZznGWt",
        "outputId": "969d7ddf-99fd-4581-82f0-d296ce6a5592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embed(pos_ids)"
      ],
      "metadata": {
        "id": "cAODAGYAwpAr",
        "outputId": "e6e77bc6-2460-4a91-98ce-3bf3a5252f3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10, 12), dtype=float32, numpy=\n",
              "array([[[-0.02627332,  0.00494404, -0.00553044,  0.00529874,\n",
              "         -0.00445414, -0.03119564,  0.04908427,  0.00199108,\n",
              "         -0.00782778, -0.00158896,  0.03763149, -0.02136957],\n",
              "        [-0.04744848, -0.00125855, -0.03048106,  0.03316036,\n",
              "         -0.00197734,  0.00174314,  0.0232848 ,  0.02844452,\n",
              "          0.04862762, -0.04100418,  0.03514743, -0.0242133 ],\n",
              "        [ 0.01328549, -0.00645056, -0.0093023 ,  0.018146  ,\n",
              "         -0.04506398, -0.03319913, -0.02150148, -0.02230905,\n",
              "          0.00856518, -0.02947686,  0.02523848, -0.03266766],\n",
              "        [ 0.00449647, -0.02950015, -0.04376494,  0.04214707,\n",
              "         -0.03811678, -0.02868011, -0.03589477,  0.03419762,\n",
              "          0.04107979,  0.02873271,  0.01633034, -0.0194487 ],\n",
              "        [ 0.02935529, -0.04177836, -0.04488919, -0.03773079,\n",
              "         -0.01054424,  0.04607414,  0.03547266,  0.00316938,\n",
              "          0.04814469, -0.0112382 , -0.0283497 , -0.00060313],\n",
              "        [-0.04935927,  0.0287292 , -0.03315626, -0.03452907,\n",
              "          0.02696041, -0.04182808, -0.03011742,  0.01208083,\n",
              "          0.00889037,  0.01043107, -0.03231344, -0.0325183 ],\n",
              "        [-0.0472222 , -0.04470063,  0.01623067, -0.00702249,\n",
              "         -0.01612936, -0.03904019, -0.01345099,  0.02311042,\n",
              "          0.01976276, -0.00274708,  0.03572085, -0.04635547],\n",
              "        [-0.02418422,  0.0211895 , -0.02034953, -0.02778579,\n",
              "          0.00171686,  0.04838279,  0.04530198, -0.00882737,\n",
              "          0.00322243,  0.04546907,  0.02781317, -0.0005365 ],\n",
              "        [ 0.0048847 ,  0.00657017, -0.03835946, -0.04080499,\n",
              "         -0.03235554,  0.02851528, -0.01078932,  0.02434916,\n",
              "          0.01370808, -0.01102678,  0.01042068, -0.04462115],\n",
              "        [-0.02526927,  0.00705897, -0.01722537, -0.00663795,\n",
              "          0.0240449 ,  0.01195043,  0.00634839, -0.01425755,\n",
              "          0.01906288, -0.00811549, -0.04773686, -0.01406661]],\n",
              "\n",
              "       [[-0.02627332,  0.00494404, -0.00553044,  0.00529874,\n",
              "         -0.00445414, -0.03119564,  0.04908427,  0.00199108,\n",
              "         -0.00782778, -0.00158896,  0.03763149, -0.02136957],\n",
              "        [-0.04744848, -0.00125855, -0.03048106,  0.03316036,\n",
              "         -0.00197734,  0.00174314,  0.0232848 ,  0.02844452,\n",
              "          0.04862762, -0.04100418,  0.03514743, -0.0242133 ],\n",
              "        [ 0.01328549, -0.00645056, -0.0093023 ,  0.018146  ,\n",
              "         -0.04506398, -0.03319913, -0.02150148, -0.02230905,\n",
              "          0.00856518, -0.02947686,  0.02523848, -0.03266766],\n",
              "        [ 0.00449647, -0.02950015, -0.04376494,  0.04214707,\n",
              "         -0.03811678, -0.02868011, -0.03589477,  0.03419762,\n",
              "          0.04107979,  0.02873271,  0.01633034, -0.0194487 ],\n",
              "        [ 0.02935529, -0.04177836, -0.04488919, -0.03773079,\n",
              "         -0.01054424,  0.04607414,  0.03547266,  0.00316938,\n",
              "          0.04814469, -0.0112382 , -0.0283497 , -0.00060313],\n",
              "        [-0.04935927,  0.0287292 , -0.03315626, -0.03452907,\n",
              "          0.02696041, -0.04182808, -0.03011742,  0.01208083,\n",
              "          0.00889037,  0.01043107, -0.03231344, -0.0325183 ],\n",
              "        [-0.0472222 , -0.04470063,  0.01623067, -0.00702249,\n",
              "         -0.01612936, -0.03904019, -0.01345099,  0.02311042,\n",
              "          0.01976276, -0.00274708,  0.03572085, -0.04635547],\n",
              "        [-0.02418422,  0.0211895 , -0.02034953, -0.02778579,\n",
              "          0.00171686,  0.04838279,  0.04530198, -0.00882737,\n",
              "          0.00322243,  0.04546907,  0.02781317, -0.0005365 ],\n",
              "        [ 0.0048847 ,  0.00657017, -0.03835946, -0.04080499,\n",
              "         -0.03235554,  0.02851528, -0.01078932,  0.02434916,\n",
              "          0.01370808, -0.01102678,  0.01042068, -0.04462115],\n",
              "        [-0.02526927,  0.00705897, -0.01722537, -0.00663795,\n",
              "          0.0240449 ,  0.01195043,  0.00634839, -0.01425755,\n",
              "          0.01906288, -0.00811549, -0.04773686, -0.01406661]],\n",
              "\n",
              "       [[-0.02627332,  0.00494404, -0.00553044,  0.00529874,\n",
              "         -0.00445414, -0.03119564,  0.04908427,  0.00199108,\n",
              "         -0.00782778, -0.00158896,  0.03763149, -0.02136957],\n",
              "        [-0.04744848, -0.00125855, -0.03048106,  0.03316036,\n",
              "         -0.00197734,  0.00174314,  0.0232848 ,  0.02844452,\n",
              "          0.04862762, -0.04100418,  0.03514743, -0.0242133 ],\n",
              "        [ 0.01328549, -0.00645056, -0.0093023 ,  0.018146  ,\n",
              "         -0.04506398, -0.03319913, -0.02150148, -0.02230905,\n",
              "          0.00856518, -0.02947686,  0.02523848, -0.03266766],\n",
              "        [ 0.00449647, -0.02950015, -0.04376494,  0.04214707,\n",
              "         -0.03811678, -0.02868011, -0.03589477,  0.03419762,\n",
              "          0.04107979,  0.02873271,  0.01633034, -0.0194487 ],\n",
              "        [ 0.02935529, -0.04177836, -0.04488919, -0.03773079,\n",
              "         -0.01054424,  0.04607414,  0.03547266,  0.00316938,\n",
              "          0.04814469, -0.0112382 , -0.0283497 , -0.00060313],\n",
              "        [-0.04935927,  0.0287292 , -0.03315626, -0.03452907,\n",
              "          0.02696041, -0.04182808, -0.03011742,  0.01208083,\n",
              "          0.00889037,  0.01043107, -0.03231344, -0.0325183 ],\n",
              "        [-0.0472222 , -0.04470063,  0.01623067, -0.00702249,\n",
              "         -0.01612936, -0.03904019, -0.01345099,  0.02311042,\n",
              "          0.01976276, -0.00274708,  0.03572085, -0.04635547],\n",
              "        [-0.02418422,  0.0211895 , -0.02034953, -0.02778579,\n",
              "          0.00171686,  0.04838279,  0.04530198, -0.00882737,\n",
              "          0.00322243,  0.04546907,  0.02781317, -0.0005365 ],\n",
              "        [ 0.0048847 ,  0.00657017, -0.03835946, -0.04080499,\n",
              "         -0.03235554,  0.02851528, -0.01078932,  0.02434916,\n",
              "          0.01370808, -0.01102678,  0.01042068, -0.04462115],\n",
              "        [-0.02526927,  0.00705897, -0.01722537, -0.00663795,\n",
              "          0.0240449 ,  0.01195043,  0.00634839, -0.01425755,\n",
              "          0.01906288, -0.00811549, -0.04773686, -0.01406661]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = [\n",
        "    \"Where can I find a pizzeria?\",\n",
        "    \"Mass hysteria over listeria.\",\n",
        "    \"I ain't no circle back girl.\"\n",
        "]\n",
        "\n",
        "bpemb_en.encode(input_batch)"
      ],
      "metadata": {
        "id": "jbX82NUpwyGL",
        "outputId": "a8c559a8-859f-4c19-c060-6b853412ffdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?'],\n",
              " ['▁mass', '▁hy', 'ster', 'ia', '▁over', '▁l', 'ister', 'ia', '.'],\n",
              " ['▁i', '▁a', 'in', \"'\", 't', '▁no', '▁circle', '▁back', '▁girl', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = bpemb_en.encode_ids(input_batch)\n",
        "print(\"Vectorized inputs:\")\n",
        "input_seqs"
      ],
      "metadata": {
        "id": "wOXHqq2Kxh5r",
        "outputId": "681f7262-1754-4431-b50a-0296a53a1e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized inputs:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[571, 280, 386, 1934, 4, 24, 248, 4339, 177, 9967],\n",
              " [1535, 1354, 1238, 177, 380, 43, 871, 177, 9935],\n",
              " [386, 4, 6, 9937, 9915, 467, 5410, 810, 3692, 9935]]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, padding=\"post\")\n",
        "print(\"Input to the encoder:\")\n",
        "print(padded_input_seqs.shape)\n",
        "print(padded_input_seqs)"
      ],
      "metadata": {
        "id": "np2vsXpwxMS8",
        "outputId": "0bcf48b6-a3bc-4811-f8a2-031a4dab217c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the encoder:\n",
            "(3, 10)\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_mask = enc_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "enc_mask"
      ],
      "metadata": {
        "id": "aYPlbsrvZu8_",
        "outputId": "5999424e-3207-4864-d90e-cbde05511029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-e1b5ed6c65ea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menc_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc_mask' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_blocks = 6\n",
        "\n",
        "# d_model is the embedding dimension used throughout.\n",
        "d_model = 12\n",
        "\n",
        "num_heads = 3\n",
        "\n",
        "# Feed-forward network hidden dimension width.\n",
        "ffn_hidden_dim = 48\n",
        "\n",
        "src_vocab_size = bpemb_vocab_size\n",
        "max_input_seq_len = padded_input_seqs.shape[1]\n",
        "\n",
        "encoder = Encoder(\n",
        "    num_encoder_blocks,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    ffn_hidden_dim,\n",
        "    src_vocab_size,\n",
        "    max_input_seq_len)"
      ],
      "metadata": {
        "id": "Ns8G5ujRVQMv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_output, attn_weights = encoder(padded_input_seqs, training=True, \n",
        "                                       mask=enc_mask)\n",
        "print(f\"Encoder output {encoder_output.shape}:\")\n",
        "print(encoder_output)"
      ],
      "metadata": {
        "id": "rf6q86hBj8eV",
        "outputId": "426891a1-b358-40ba-a53b-4e16771b6991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-65cf72c529b8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder_output, attn_weights = encoder(padded_input_seqs, training=True, \n\u001b[0;32m----> 2\u001b[0;31m                                        mask=enc_mask)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Encoder output {encoder_output.shape}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc_mask' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Block"
      ],
      "metadata": {
        "id": "24TYaX3zMNAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  # Note the decoder block takes two masks. One for the first MHSA, another\n",
        "  # for the second MHSA.\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
        "    mhsa_output1 = self.dropout1(mhsa_output1, training=training)\n",
        "    mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
        "\n",
        "    mhsa_output2, attn_weights = self.mhsa2(mhsa_output1, encoder_output, \n",
        "                                            encoder_output, \n",
        "                                            memory_mask)\n",
        "    mhsa_output2 = self.dropout2(mhsa_output2, training=training)\n",
        "    mhsa_output2 = self.layernorm2(mhsa_output2 + mhsa_output1)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    output = self.layernorm3(ffn_output + mhsa_output2)\n",
        "\n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "Hco1IwfutNqD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "YVstTioxMNDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    token_embeds = self.token_embed(target)\n",
        "\n",
        "    # Generate position indices.\n",
        "    num_pos = target.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, target.shape)\n",
        "\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "27zG_wV3MNJ_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Made up values.\n",
        "target_input_seqs = [\n",
        "    [1, 652, 723, 123, 62],\n",
        "    [1, 25,  98, 129, 248, 215, 359, 249],\n",
        "    [1, 2369, 1259, 125, 486],\n",
        "]"
      ],
      "metadata": {
        "id": "0X6gKNzgv0gP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_target_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_input_seqs, padding=\"post\")\n",
        "print(\"Padded target inputs to the decoder:\")\n",
        "print(padded_target_input_seqs.shape)\n",
        "print(padded_target_input_seqs)"
      ],
      "metadata": {
        "id": "4hFp1nkSypnz",
        "outputId": "112b36a9-5075-4f49-e528-e0d78b8b9ff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded target inputs to the decoder:\n",
            "(3, 8)\n",
            "[[   1  652  723  123   62    0    0    0]\n",
            " [   1   25   98  129  248  215  359  249]\n",
            " [   1 2369 1259  125  486    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create the padding mask the same way we did for the encoder."
      ],
      "metadata": {
        "id": "qZysfgvUzNBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_padding_mask = tf.cast(tf.math.not_equal(padded_target_input_seqs, 0), tf.float32)\n",
        "dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(dec_padding_mask)"
      ],
      "metadata": {
        "id": "PLKeI4R20axA",
        "outputId": "93a2807a-da6a-409c-d9c4-1b14deb96947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 1, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_input_seq_len = padded_target_input_seqs.shape[1]\n",
        "look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len, \n",
        "                                               target_input_seq_len)), -1, 0)\n",
        "print(look_ahead_mask)"
      ],
      "metadata": {
        "id": "yZFnGgJa04a-",
        "outputId": "8b5046fa-e502-4d65-b2f6-8009ea565a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1.]], shape=(8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
        "print(\"The decoder mask:\")\n",
        "print(dec_mask)"
      ],
      "metadata": {
        "id": "vArTOY1x2bzn",
        "outputId": "40a4815b-cbd7-4406-baaa-6180fe938cd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decoder mask:\n",
            "tf.Tensor(\n",
            "[[[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(6, 12, 3, 48, 10000, 8)\n",
        "decoder_output, _ = decoder(encoder_output, padded_target_input_seqs, \n",
        "                            True, dec_mask, enc_mask)\n",
        "print(f\"Decoder output {decoder_output.shape}:\")\n",
        "print(decoder_output)"
      ],
      "metadata": {
        "id": "bFE-VaCrmLKu",
        "outputId": "e5c24de8-43f3-4e4d-c79b-fdd5045ed10c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-9933b0866ef5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m decoder_output, _ = decoder(encoder_output, padded_target_input_seqs, \n\u001b[0m\u001b[1;32m      3\u001b[0m                             True, dec_mask, enc_mask)\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Decoder output {decoder_output.shape}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder_output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "UgFtxMQxMNNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the pieces to build the **Transformer** itself, and it's pretty simple. "
      ],
      "metadata": {
        "id": "bYFJuqbl-Jt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, source_vocab_size,\n",
        "               target_vocab_size, max_input_len, max_target_len, dropout_rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, source_vocab_size, \n",
        "                           max_input_len, dropout_rate)\n",
        "    \n",
        "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "                           max_target_len, dropout_rate)\n",
        "    \n",
        "    # The final dense layer to generate logits from the decoder output.\n",
        "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, input_seqs, target_input_seqs, training, encoder_mask,\n",
        "           decoder_mask, memory_mask):\n",
        "    encoder_output, encoder_attn_weights = self.encoder(input_seqs, \n",
        "                                                        training, encoder_mask)\n",
        "\n",
        "    decoder_output, decoder_attn_weights = self.decoder(encoder_output, \n",
        "                                                        target_input_seqs, training,\n",
        "                                                        decoder_mask, memory_mask)\n",
        "\n",
        "    return self.output_layer(decoder_output), encoder_attn_weights, decoder_attn_weights\n"
      ],
      "metadata": {
        "id": "DfNkAsv8MNQ8"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_blocks = 6,\n",
        "    d_model = 12,\n",
        "    num_heads = 3,\n",
        "    hidden_dim = 48,\n",
        "    source_vocab_size = bpemb_vocab_size,\n",
        "    target_vocab_size = 7000, # made-up target vocab size.\n",
        "    max_input_len = padded_input_seqs.shape[1],\n",
        "    max_target_len = padded_target_input_seqs.shape[1])\n",
        "\n",
        "transformer_output, _, _ = transformer(padded_input_seqs, \n",
        "                                       padded_target_input_seqs, True, \n",
        "                                       enc_mask, dec_mask, memory_mask=enc_mask)\n",
        "print(f\"Transformer output {transformer_output.shape}:\")\n",
        "print(transformer_output) # If training, we would use this output to calculate losses."
      ],
      "metadata": {
        "id": "1VOou7zjQ7el",
        "outputId": "8a3a15a5-17fc-4dca-dff5-85ef4191e29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-a1b9d672c853>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m transformer_output, _, _ = transformer(padded_input_seqs, \n\u001b[1;32m     12\u001b[0m                                        \u001b[0mpadded_target_input_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                        enc_mask, dec_mask, memory_mask=enc_mask)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Transformer output {transformer_output.shape}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# If training, we would use this output to calculate losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc_mask' is not defined"
          ]
        }
      ]
    }
  ]
}